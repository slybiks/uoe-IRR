@inproceedings{Shen2020DialogXLAX,
  title={DialogXL: All-in-One XLNet for Multi-Party Conversation Emotion Recognition},
  author={Weizhou Shen and Junqing Chen and Xiaojun Quan and Zhixiang Xie},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2020}
}

@article{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={ArXiv},
  year={2019},
  volume={abs/1810.04805}
}

@inproceedings{Yang2019XLNetGA,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Zhilin Yang and Zihang Dai and Yiming Yang and Jaime G. Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
  booktitle={Neural Information Processing Systems},
  year={2019}
}

@article{Majumder2018DialogueRNNAA,
  title={DialogueRNN: An Attentive RNN for Emotion Detection in Conversations},
  author={Navonil Majumder and Soujanya Poria and Devamanyu Hazarika and Rada Mihalcea and Alexander Gelbukh and E. Cambria},
  journal={ArXiv},
  year={2018},
  volume={abs/1811.00405}
}

@inproceedings{Ghosal2019DialogueGCNAG,
  title={DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation},
  author={Deepanway Ghosal and Navonil Majumder and Soujanya Poria and Niyati Chhaya and Alexander Gelbukh},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2019}
}

@article{Dai2019TransformerXLAL,
  title={Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  author={Zihang Dai and Zhilin Yang and Yiming Yang and Jaime G. Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
  journal={ArXiv},
  year={2019},
  volume={abs/1901.02860}
}

@article{Li2021EnhancingEI,
  title={Enhancing emotion inference in conversations with commonsense knowledge},
  author={Dayu Li and Xiaodan Zhu and Yang Li and Suge Wang and Deyu Li and Jian Liao and Jianxing Zheng},
  journal={Knowl. Based Syst.},
  year={2021},
  volume={232},
  pages={107449}
}

@article{Busso2008IEMOCAPIE,
  title={IEMOCAP: interactive emotional dyadic motion capture database},
  author={Carlos Busso and Murtaza Bulut and Chi-Chun Lee and Ebrahim (Abe) Kazemzadeh and Emily Mower Provost and Samuel Kim and Jeannette N. Chang and Sungbok Lee and Shrikanth S. Narayanan},
  journal={Language Resources and Evaluation},
  year={2008},
  volume={42},
  pages={335-359}
}

@article{Poria2018MELDAM,
  title={MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations},
  author={Soujanya Poria and Devamanyu Hazarika and Navonil Majumder and Gautam Naik and E. Cambria and Rada Mihalcea},
  journal={ArXiv},
  year={2018},
  volume={abs/1810.02508}
}

@article{Zahiri2017EmotionDO,
  title={Emotion Detection on TV Show Transcripts with Sequence-based Convolutional Neural Networks},
  author={Sayyed M. Zahiri and Jinho D. Choi},
  journal={ArXiv},
  year={2017},
  volume={abs/1708.04299}
}

@inproceedings{hazarika-etal-2018-icon,
    title = "{ICON}: Interactive Conversational Memory Network for Multimodal Emotion Detection",
    author = "Hazarika, Devamanyu  and
      Poria, Soujanya  and
      Mihalcea, Rada  and
      Cambria, Erik  and
      Zimmermann, Roger",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1280",
    doi = "10.18653/v1/D18-1280",
    pages = "2594--2604",
    abstract = "Emotion recognition in conversations is crucial for building empathetic machines. Present works in this domain do not explicitly consider the inter-personal influences that thrive in the emotional dynamics of dialogues. To this end, we propose Interactive Conversational memory Network (ICON), a multimodal emotion detection framework that extracts multimodal features from conversational videos and hierarchically models the self- and inter-speaker emotional influences into global memories. Such memories generate contextual summaries which aid in predicting the emotional orientation of utterance-videos. Our model outperforms state-of-the-art networks on multiple classification and regression tasks in two benchmark datasets.",
}

@article{10.1016/j.knosys.2019.105084,
author = {Li, Dayu and Li, Yang and Wang, Suge},
title = {Interactive Double States Emotion Cell Model for Textual Dialogue Emotion Prediction},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {189},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105084},
doi = {10.1016/j.knosys.2019.105084},
journal = {Know.-Based Syst.},
month = {feb},
numpages = {11},
keywords = {Emotion prediction, Data-driven, Macro-average F1 score, IDS-ECM, Emotion recognition, Textual dialogue, Human–computer interaction}
}

@article{Speer2016ConceptNet5A,
  title={ConceptNet 5.5: An Open Multilingual Graph of General Knowledge},
  author={Robyn Speer and Joshua Chin and Catherine Havasi},
  journal={ArXiv},
  year={2016},
  volume={abs/1612.03975}
}

@inproceedings{10.1609/aaai.v33i01.33013027,
author = {Sap, Maarten and Le Bras, Ronan and Allaway, Emily and Bhagavatula, Chandra and Lourie, Nicholas and Rashkin, Hannah and Roof, Brendan and Smith, Noah A. and Choi, Yejin},
title = {ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33013027},
doi = {10.1609/aaai.v33i01.33013027},
abstract = {We present ATOMIC, an atlas of everyday commonsense reasoning, organized through 877k textual descriptions of inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed if-then relations with variables (e.g., "if X pays Y a compliment, then Y will likely return the compliment"). We propose nine if-then relation types to distinguish causes vs. effects, agents vs. themes, voluntary vs. involuntary events, and actions vs. mental states. By gen-eratively training on the rich inferential knowledge described in ATOMIC, we show that neural models can acquire simple commonsense capabilities and reason about previously unseen events. Experimental results demonstrate that multitask models that incorporate the hierarchical structure of if-then relation types lead to more accurate inference compared to models trained in isolation, as measured by both automatic and human evaluation.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {372},
numpages = {9},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{bosselut-etal-2019-comet,
    title = "{COMET}: Commonsense Transformers for Automatic Knowledge Graph Construction",
    author = "Bosselut, Antoine  and
      Rashkin, Hannah  and
      Sap, Maarten  and
      Malaviya, Chaitanya  and
      Celikyilmaz, Asli  and
      Choi, Yejin",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1470",
    doi = "10.18653/v1/P19-1470",
    pages = "4762--4779",
    abstract = "We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5{\%} (ATOMIC) and 91.7{\%} (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.",
}

@inproceedings{pennington-etal-2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@misc{https://doi.org/10.48550/arxiv.1907.11692,
  doi = {10.48550/ARXIV.1907.11692},
  url = {https://arxiv.org/abs/1907.11692},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Hochreiter1997LongSM,
  title={Long Short-Term Memory},
  author={Sepp Hochreiter and J{\"u}rgen Schmidhuber},
  journal={Neural Computation},
  year={1997},
  volume={9},
  pages={1735-1780}
}

@inproceedings{hasegawa-etal-2013-predicting,
    title = "Predicting and Eliciting Addressee{'}s Emotion in Online Dialogue",
    author = "Hasegawa, Takayuki  and
      Kaji, Nobuhiro  and
      Yoshinaga, Naoki  and
      Toyoda, Masashi",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P13-1095",
    pages = "964--972",
}

@article{Majumder_Poria_Hazarika_Mihalcea_Gelbukh_Cambria_2019, title={DialogueRNN: An Attentive RNN for Emotion Detection in Conversations}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4657}, DOI={10.1609/aaai.v33i01.33016818}, abstractNote={&lt;p&gt;Emotion detection in conversations is a necessary step for a number of applications, including opinion mining over chat history, social media threads, debates, argumentation mining, understanding consumer feedback in live conversations, and so on. Currently systems do not treat the parties in the conversation individually by adapting to the speaker of each utterance. In this paper, we describe a new method based on recurrent neural networks that keeps track of the individual party states throughout the conversation and uses this information for emotion classification. Our model outperforms the state-of-the-art by a significant margin on two different datasets.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Majumder, Navonil and Poria, Soujanya and Hazarika, Devamanyu and Mihalcea, Rada and Gelbukh, Alexander and Cambria, Erik}, year={2019}, month={Jul.}, pages={6818-6825} }

@article{Poria2019EmotionRI,
  title={Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances},
  author={Soujanya Poria and Navonil Majumder and Rada Mihalcea and Eduard H. Hovy},
  journal={IEEE Access},
  year={2019},
  volume={7},
  pages={100943-100953}
}

@inproceedings{hazarika-etal-2018-icon,
    title = "{ICON}: Interactive Conversational Memory Network for Multimodal Emotion Detection",
    author = "Hazarika, Devamanyu  and
      Poria, Soujanya  and
      Mihalcea, Rada  and
      Cambria, Erik  and
      Zimmermann, Roger",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1280",
    doi = "10.18653/v1/D18-1280",
    pages = "2594--2604",
    abstract = "Emotion recognition in conversations is crucial for building empathetic machines. Present works in this domain do not explicitly consider the inter-personal influences that thrive in the emotional dynamics of dialogues. To this end, we propose Interactive COnversational memory Network (ICON), a multimodal emotion detection framework that extracts multimodal features from conversational videos and hierarchically models the self- and inter-speaker emotional influences into global memories. Such memories generate contextual summaries which aid in predicting the emotional orientation of utterance-videos. Our model outperforms state-of-the-art networks on multiple classification and regression tasks in two benchmark datasets.",
}

@inproceedings{poria-etal-2017-context,
    title = "Context-Dependent Sentiment Analysis in User-Generated Videos",
    author = "Poria, Soujanya  and
      Cambria, Erik  and
      Hazarika, Devamanyu  and
      Majumder, Navonil  and
      Zadeh, Amir  and
      Morency, Louis-Philippe",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1081",
    doi = "10.18653/v1/P17-1081",
    pages = "873--883",
    abstract = "Multimodal sentiment analysis is a developing area of research, which involves the identification of sentiments in videos. Current research considers utterances as independent entities, i.e., ignores the interdependencies and relations among the utterances of a video. In this paper, we propose a LSTM-based model that enables utterances to capture contextual information from their surroundings in the same video, thus aiding the classification process. Our method shows 5-10{\%} performance improvement over the state of the art and high robustness to generalizability.",
}

@inproceedings{10.1145/3136755.3136801,
author = {Chen, Minghai and Wang, Sen and Liang, Paul Pu and Baltru\v{s}aitis, Tadas and Zadeh, Amir and Morency, Louis-Philippe},
title = {Multimodal Sentiment Analysis with Word-Level Fusion and Reinforcement Learning},
year = {2017},
isbn = {9781450355438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136755.3136801},
doi = {10.1145/3136755.3136801},
abstract = {With the increasing popularity of video sharing websites such as YouTube and Facebook, multimodal sentiment analysis has received increasing attention from the scientific community. Contrary to previous works in multimodal sentiment analysis which focus on holistic information in speech segments such as bag of words representations and average facial expression intensity, we propose a novel deep architecture for multimodal sentiment analysis that is able to perform modality fusion at the word level. In this paper, we propose the Gated Multimodal Embedding LSTM with Temporal Attention (GME-LSTM(A)) model that is composed of 2 modules. The Gated Multimodal Embedding allows us to alleviate the difficulties of fusion when there are noisy modalities. The LSTM with Temporal Attention can perform word level fusion at a finer fusion resolution between the input modalities and attends to the most important time steps. As a result, the GME-LSTM(A) is able to better model the multimodal structure of speech through time and perform better sentiment comprehension. We demonstrate the effectiveness of this approach on the publicly-available Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis (CMU-MOSI) dataset by achieving state-of-the-art sentiment classification and regression results. Qualitative analysis on our model emphasizes the importance of the Temporal Attention Layer in sentiment prediction because the additional acoustic and visual modalities are noisy. We also demonstrate the effectiveness of the Gated Multimodal Embedding in selectively filtering these noisy modalities out. These results and analysis open new areas in the study of sentiment analysis in human communication and provide new models for multimodal fusion.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimodal Interaction},
pages = {163–171},
numpages = {9},
keywords = {Deep Learning, Multimodal Fusion, Reinforcement Learning, Human Communication, Multimodal Sentiment Analysis},
location = {Glasgow, UK},
series = {ICMI '17}
}

@article{Hazarika2019ConversationalTL,
  title={Conversational transfer learning for emotion recognition},
  author={Devamanyu Hazarika and Soujanya Poria and Roger Zimmermann and Rada Mihalcea},
  journal={Inf. Fusion},
  year={2019},
  volume={65},
  pages={1-12}
}

@inbook{Weigand+1998+35+48,
url = {https://doi.org/10.1515/9783110965056-005},
title = {Emotions in Dialogue},
booktitle = {Dialoganalyse VI/1},
booktitle = {Referate der 6. Arbeitstagung, Prag 1996},
author = {Edda Weigand},
editor = {Svetla Cmejrková and Jana Hoffmannová and Olga Müllerová},
publisher = {Max Niemeyer Verlag},
address = {Berlin, Boston},
pages = {35--48},
doi = {doi:10.1515/9783110965056-005},
isbn = {9783110965056},
year = {1998},
lastchecked = {2023-01-28}
}

@article{Koval2012ChangingED,
  title={Changing emotion dynamics: individual differences in the effect of anticipatory social stress on emotional inertia.},
  author={Peter Koval and Peter Kuppens},
  journal={Emotion},
  year={2012},
  volume={12 2},
  pages={
          256-67
        }
}

@inproceedings{Navarretta2016MirroringFE,
  title={Mirroring Facial Expressions and Emotions in Dyadic Conversations},
  author={Costanza Navarretta},
  booktitle={International Conference on Language Resources and Evaluation},
  year={2016}
}

@inproceedings{Serban2015BuildingED,
  title={Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models},
  author={Iulian Serban and Alessandro Sordoni and Yoshua Bengio and Aaron C. Courville and Joelle Pineau},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2015}
}

@inproceedings{Shimizu2018PretrainingSC,
  title={Pretraining Sentiment Classifiers with Unlabeled Dialog Data},
  author={Toru Shimizu and N. Shimizu and Hayato Kobayashi},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2018}
}

@article{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={ArXiv},
  year={2019},
  volume={abs/1810.04805}
}

@inproceedings{Cho2014LearningPR,
  title={Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation},
  author={Kyunghyun Cho and Bart van Merrienboer and Çaglar G{\"u}lçehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2014}
}

@misc{beam-decoding,
  title = {{Beam-Decoding}},
  howpublished = {\url{https://github.com/ctr4si/}},
  note = {Accessed: 2023-01-29}
}

@inproceedings{DanescuNiculescuMizil2011ChameleonsII,
  title={Chameleons in Imagined Conversations: A New Approach to Understanding Coordination of Linguistic Style in Dialogs},
  author={Cristian Danescu-Niculescu-Mizil and Lillian Lee},
  booktitle={CMCL@ACL},
  year={2011}
}

@inproceedings{Lowe2015TheUD,
  title={The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems},
  author={Ryan Lowe and Nissan Pow and Iulian Serban and Joelle Pineau},
  booktitle={SIGDIAL Conference},
  year={2015}
}

@article{Park2018AHL,
  title={A Hierarchical Latent Structure for Variational Conversation Modeling},
  author={Yookoon Park and Jaemin Cho and Gunhee Kim},
  journal={ArXiv},
  year={2018},
  volume={abs/1804.03424}
}

@inproceedings{Li2017DailyDialogAM,
  title={DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset},
  author={Yanran Li and Hui Su and Xiaoyu Shen and Wenjie Li and Ziqiang Cao and Shuzi Niu},
  booktitle={International Joint Conference on Natural Language Processing},
  year={2017}
}

@ARTICLE{5959155,
  author={McKeown, Gary and Valstar, Michel and Cowie, Roddy and Pantic, Maja and Schroder, Marc},
  journal={IEEE Transactions on Affective Computing}, 
  title={The SEMAINE Database: Annotated Multimodal Records of Emotionally Colored Conversations between a Person and a Limited Agent}, 
  year={2012},
  volume={3},
  number={1},
  pages={5-17},
  doi={10.1109/T-AFFC.2011.20}}

@inproceedings{Kim2014ConvolutionalNN,
  title={Convolutional Neural Networks for Sentence Classification},
  author={Yoon Kim},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2014}
}

@inproceedings{Sukhbaatar2015EndToEndMN,
  title={End-To-End Memory Networks},
  author={Sainbayar Sukhbaatar and Arthur D. Szlam and Jason Weston and Rob Fergus},
  booktitle={NIPS},
  year={2015}
}

@article{Poria2017MultilevelMA,
  title={Multi-level Multiple Attentions for Contextual Multimodal Sentiment Analysis},
  author={Soujanya Poria and E. Cambria and Devamanyu Hazarika and Navonil Majumder and Amir Zadeh and Louis-Philippe Morency},
  journal={2017 IEEE International Conference on Data Mining (ICDM)},
  year={2017},
  pages={1033-1038}
}

@article{Hazarika2018ConversationalMN,
  title={Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos},
  author={Devamanyu Hazarika and Soujanya Poria and Amir Zadeh and E. Cambria and Louis-Philippe Morency and Roger Zimmermann},
  journal={Proceedings of the conference. Association for Computational Linguistics. North American Chapter. Meeting},
  year={2018},
  volume={2018},
  pages={
          2122-2132
        }
}

@article{Mohammad2013CROWDSOURCINGAW,
  title={Crowdsourcing a word–emotion association lexicon},
  author={Saif M. Mohammad and Peter D. Turney},
  journal={Computational Intelligence},
  year={2013},
  volume={29}
}

@article{Tu2022ContextAS,
  title={Context- and Sentiment-Aware Networks for Emotion Recognition in Conversation},
  author={Geng Tu and Jintao Wen and Cheng Liu and Dazhi Jiang and E. Cambria},
  journal={IEEE Transactions on Artificial Intelligence},
  year={2022},
  volume={3},
  pages={699-708}
}

@inproceedings{Ekkekakis2013TheMO,
  title={The Measurement of Affect, Mood, and Emotion: A Guide for Health-Behavioral Research},
  author={Panteleimon Ekkekakis and James A. Russell},
  year={2013}
}

@article{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  journal={ArXiv},
  year={2017},
  volume={abs/1706.03762}
}

@article{Tang2018WhySA,
  title={Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures},
  author={Gongbo Tang and Mathias M{\"u}ller and Annette Rios Gonzales and Rico Sennrich},
  journal={ArXiv},
  year={2018},
  volume={abs/1808.08946}
}

@article{Gunel2020SupervisedCL,
  title={Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning},
  author={Beliz Gunel and Jingfei Du and Alexis Conneau and Ves Stoyanov},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.01403}
}

@article{Khosla2020SupervisedCL,
  title={Supervised Contrastive Learning},
  author={Prannay Khosla and Piotr Teterwak and Chen Wang and Aaron Sarna and Yonglong Tian and Phillip Isola and Aaron Maschinot and Ce Liu and Dilip Krishnan},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.11362}
}

@article{Cambria2020SenticNet6E,
  title={SenticNet 6: Ensemble Application of Symbolic and Subsymbolic AI for Sentiment Analysis},
  author={E. Cambria and Yang Li and Frank Z. Xing and Soujanya Poria and Kenneth Kwok},
  journal={Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  year={2020}
}

@article{Mehrabian1996PleasurearousaldominanceAG,
  title={Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in Temperament},
  author={Albert Mehrabian},
  journal={Current Psychology},
  year={1996},
  volume={14},
  pages={261-292}
}

@article{Susanto2020TheHM,
  title={The Hourglass Model Revisited},
  author={Yosephine Susanto and Andrew G. Livingstone and Bee Chin Ng and E. Cambria},
  journal={IEEE Intelligent Systems},
  year={2020},
  volume={35},
  pages={96-102}
}

@article{Chatterjee2019UnderstandingEI,
  title={Understanding Emotions in Text Using Deep Learning and Big Data},
  author={Ankush Chatterjee and Umang Gupta and Manoj Kumar Chinnakotla and Radhakrishnan Srikanth and Michel Galley and Puneet Agrawal},
  journal={Comput. Hum. Behav.},
  year={2019},
  volume={93},
  pages={309-317}
}

@inproceedings{Mohammad2018ObtainingRH,
  title={Obtaining Reliable Human Ratings of Valence, Arousal, and Dominance for 20,000 English Words},
  author={Saif M. Mohammad},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2018}
}

@inproceedings{Zhong2019KnowledgeEnrichedTF,
  title={Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations},
  author={Peixiang Zhong and Di Wang and Chunyan Miao},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2019}
}

@inproceedings{Ishiwatari2020RelationawareGA,
  title={Relation-aware Graph Attention Networks with Relational Position Encodings for Emotion Recognition in Conversations},
  author={Taichi Ishiwatari and Y. Yasuda and Taro Miyazaki and Jun Goto},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2020}
}

@article{Feng2021SINNAS,
  title={SINN: A speaker influence aware neural network model for emotion detection in conversations},
  author={Shi Feng and Jia Wei and Daling Wang and Xiaocui Yang and Zhenfei Yang and Yifei Zhang and Ge Yu},
  journal={World Wide Web},
  year={2021},
  volume={24},
  pages={2019 - 2048}
}

@article{Ma2021AMN,
  title={A multi-view network for real-time emotion recognition in conversations},
  author={Hui Ma and Jian Wang and Hongfei Lin and Xuejun Pan and Yijia Zhang and Zhihao Yang},
  journal={Knowl. Based Syst.},
  year={2021},
  volume={236},
  pages={107751}
}

@article{Jiao2020ExploitingUD,
  title={Exploiting Unsupervised Data for Emotion Recognition in Conversations},
  author={Wenxiang Jiao and Michael R. Lyu and I. King},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.01908}
}

@inproceedings{Jiao2019HiGRUHG,
  title={HiGRU: Hierarchical Gated Recurrent Units for Utterance-Level Emotion Recognition},
  author={Wenxiang Jiao and Haiqin Yang and Irwin King and Michael R. Lyu},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019}
}

@inproceedings{Bao2019PLATOPD,
  title={PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable},
  author={Siqi Bao and H. He and Fan Wang and Hua Wu},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}

@inproceedings{Ham2020EndtoEndNP,
  title={End-to-End Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2},
  author={Dong-hyun Ham and Jeong-Gwan Lee and Youngsoo Jang and Kyungmin Kim},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2020}
}

@inproceedings{Ghosal2020COSMICCK,
  title={COSMIC: COmmonSense knowledge for eMotion Identification in Conversations},
  author={Deepanway Ghosal and Navonil Majumder and Alexander Gelbukh and Rada Mihalcea and Soujanya Poria},
  booktitle={Findings},
  year={2020}
}

@article{Serban2016AHL,
  title={A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues},
  author={Iulian Serban and Alessandro Sordoni and Ryan Lowe and Laurent Charlin and Joelle Pineau and Aaron C. Courville and Yoshua Bengio},
  journal={ArXiv},
  year={2016},
  volume={abs/1605.06069}
}

@article{Provost2011AFF,
  title={A Framework for Automatic Human Emotion Classification Using Emotion Profiles},
  author={Emily Mower Provost and Maja J. Matari{\'c} and Shrikanth S. Narayanan},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  year={2011},
  volume={19},
  pages={1057-1070}
}

@inproceedings{Li2015ADO,
  title={A Diversity-Promoting Objective Function for Neural Conversation Models},
  author={Jiwei Li and Michel Galley and Chris Brockett and Jianfeng Gao and William B. Dolan},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2015}
}

@article{Zhou2017EmotionalCM,
  title={Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory},
  author={Hao Zhou and Minlie Huang and Tianyang Zhang and Xiaoyan Zhu and Bing-Qian Liu},
  journal={ArXiv},
  year={2017},
  volume={abs/1704.01074}
}

@article{Li2018DeepFE,
  title={Deep Facial Expression Recognition: A Survey},
  author={Shan Li and Weihong Deng},
  journal={IEEE Transactions on Affective Computing},
  year={2018},
  volume={13},
  pages={1195-1215}
}

@inproceedings{Drakopoulos2019EmotionRF,
  title={Emotion Recognition from Speech: A Survey},
  author={Georgios Drakopoulos and George Pikramenos and Evangelos D. Spyrou and Stavros J. Perantonis},
  booktitle={International Conference on Web Information Systems and Technologies},
  year={2019}
}

@inproceedings{Marchal2019SurveyOA,
  title={Survey on AI-Based Multimodal Methods for Emotion Detection},
  author={Catherine Mar{\'e}chal and Dariusz Mikołajewski and Krzysztof Tyburek and Piotr Prokopowicz and Lamine Bougueroua and Corinne Ancourt and Katarzyna Wegrzyn-Wolska},
  booktitle={High-Performance Modelling and Simulation for Big Data Applications},
  year={2019}
}

@inproceedings{Strapparava2004WordNetAA,
  title={WordNet Affect: an Affective Extension of WordNet},
  author={Carlo Strapparava and Alessandro Valitutti},
  booktitle={International Conference on Language Resources and Evaluation},
  year={2004}
}

@article{Wang2012HarnessingT,
  title={Harnessing Twitter "Big Data" for Automatic Emotion Identification},
  author={Wenbo Wang and Lu Chen and Krishnaprasad Thirunarayan and A. Sheth},
  journal={2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing},
  year={2012},
  pages={587-592}
}

@inproceedings{Liu2021LifelongAC,
  title={Lifelong and Continual Learning Dialogue Systems: Learning during Conversation},
  author={Bing Liu and S. Mazumder},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2021}
}

@article{Tu2021ExplorationME,
  title={Exploration meets exploitation: Multitask learning for emotion recognition based on discrete and dimensional models},
  author={Geng Tu and Jintao Wen and Hao-Ying Liu and Sentao Chen and Lin Zheng and Dazhi Jiang},
  journal={Knowl. Based Syst.},
  year={2021},
  volume={235},
  pages={107598}
}

@article{Chen2017ASO,
  title={A Survey on Dialogue Systems: Recent Advances and New Frontiers},
  author={Hongshen Chen and Xiaorui Liu and Dawei Yin and Jiliang Tang},
  journal={ArXiv},
  year={2017},
  volume={abs/1711.01731}
}

@article{Poria2020BeneathTT,
  title={Beneath the Tip of the Iceberg: Current Challenges and New Directions in Sentiment Analysis Research},
  author={Soujanya Poria and Devamanyu Hazarika and Navonil Majumder and Rada Mihalcea},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.00357}
}

@article{Bradbury2016QuasiRecurrentNN,
  title={Quasi-Recurrent Neural Networks},
  author={James Bradbury and Stephen Merity and Caiming Xiong and Richard Socher},
  journal={ArXiv},
  year={2016},
  volume={abs/1611.01576}
}

@article{Bowman2015ALA,
  title={A large annotated corpus for learning natural language inference},
  author={Samuel R. Bowman and Gabor Angeli and Christopher Potts and Christopher D. Manning},
  journal={ArXiv},
  year={2015},
  volume={abs/1508.05326}
}

@inproceedings{DavalFrerot2018EpitaAS,
  title={Epita at SemEval-2018 Task 1: Sentiment Analysis Using Transfer Learning Approach},
  author={Guillaume Daval-Frerot and Abdessalam Bouchekif and Ana Maria Souza dos Santos Moreau},
  booktitle={International Workshop on Semantic Evaluation},
  year={2018}
}

@article{Ma2020ASO,
  title={A survey on empathetic dialogue systems},
  author={Yukun Ma and Khanh Linh Nguyen and Frank Z. Xing and E. Cambria},
  journal={Inf. Fusion},
  year={2020},
  volume={64},
  pages={50-70}
}

@article{Gao2022EmotionRI,
  title={Emotion recognition in conversations with emotion shift detection based on multi-task learning},
  author={Qingqing Gao and Biwei Cao and Xin Guan and Tianyun Gu and Xing Bao and Junyan Wu and Bo Liu and Jiuxin Cao},
  journal={Knowl. Based Syst.},
  year={2022},
  volume={248},
  pages={108861}
}

@article{Ma2022EmotionRW,
  title={Emotion Recognition with Conversational Generation Transfer},
  author={Hongchao Ma and Zhongqing Wang and Xiabing Zhou and Guodong Zhou and Qing Qing Zhou},
  journal={Transactions on Asian and Low-Resource Language Information Processing},
  year={2022},
  volume={21},
  pages={1 - 17}
}

@article{Fu2022ContextAK,
  title={Context- and Knowledge-Aware Graph Convolutional Network for Multimodal Emotion Recognition},
  author={Yahui Fu and Shogo Okada and Longbiao Wang and Lili Guo and Yaodong Song and Jiaxing Liu and Jianwu Dang},
  journal={IEEE MultiMedia},
  year={2022},
  volume={29},
  pages={91-100}
}