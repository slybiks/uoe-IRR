\\
Tu et. al. attempted to solve a specific sort of issue connected with integrating common sense aspects for the purpose of ERC by building on earlier work. While previous studies dealt with addressee-aware models that sought to replicate contextual speaker dependencies infused with insights from common-sense knowledge bases, the authors discovered a gap in applying these relationships without accounting for the sentimental correctness between the target words and this information. They argued that it made no difference how the words were expressed when common-sense knowledge was inappropriately inserted into target utterances. Their inspiration originated from the actuality that remembered details could shift based on emotional influence. This idea, referred to as mood congruence effects \cite{Ekkekakis2013TheMO}, caused the authors to speculate that the depiction of common-sense in an ERC system may also be affected by feelings, which is a fluctuating process. Thus, a model that incorporated mood congruence effects not only considered the link between common-sense and contextual information, but also had the tendency to choose common-sense qualities that truly aligned with the sentiment expressed by the related words. Furthermore, to quantify the extent of this agreement, the utilisation of emotional intensity was also necessary. To that end, they developed \textit{Sentic GAT}, a framework that took both context and sentiment into account \cite{Tu2022ContextAS}. There were three primary parts to the framework and each of them are reviewed in the following paragraphs.

A dialogue transformer (DT) was used to record context-specific dependencies both within and between utterances. This network took advantage of the transformer's \cite{Vaswani2017AttentionIA} ability to model continuous interdependencies and semantic derivations \cite{Tang2018WhySA}. In order to represent historical contextual aspects of the target utterance instantaneously, the DT also used a hierarchical multi-head attention (HMAT) approach. To improve context approximations, the DT handled two separate vectors that performed context inference simultaneously throughout the training phase. While one of these encoded the context information, the other that did not. The two vectors were then given to a classifier, where a positive output of 1 signified a context-sensitive utterance and a value of 0 suggested a context-free utterance. To distinguish between these two types of samples, the authors investigated a contrastive-loss \cite{Gunel2020SupervisedCL, Khosla2020SupervisedCL} that separated samples with different labels while combining those with the same labels, allowing the distinction of context-free and context-sensitive vector representations.

The authors created a CSAGAT mechanism for incorporating sentimentally and conceptually congruent common-sense knowledge into the model. There were two aspects to this -- a) To preserve the sentiment consistency between the non-functional words in an utterance and their associated external concepts, and b) To understand the relationship between the external concepts and the context of the target utterance. The authors created a context-enriched representation of each utterance by modelling two types of weights. These weights were designed to reflect the role of context and sentiment awareness. More formally, they noted that the weight between tokens and concepts was influenced by both semantic similarity and the context-related information. They also suggested that the weight between tokens and concepts depended in part on the current sentiment of the speaker. This idea was based on the phenomenon of mood congruence. They also relied on describing the consistency of a sentiment, in terms of emotional intensity, which influenced the attention weights assigned to each relationship. The authors leveraged context awareness to incorporate external concepts into the model in a dynamic manner. The weight assigned to the external concepts was not only based on the assigned confidence score, but also adjusted based on the similarity between the concepts and the surrounding context information. As a result, when context changes were encountered, the same concepts were assigned different weights, leading to different representations of knowledge. The representation of the contextual utterances was determined by normalising and summing up their weighted n-gram representations. To arrive at the attention weights, the similarity between the concepts and the previously computed contextual-representations was evaluated while accounting for the confidence score. The weights of external concepts were adjusted depending on the sentiment of the current words and concepts. This was expressed by semantic awareness. To account for this, a threshold was established to decrease the weight of concepts that were inconsistent with the sentiment in the target utterance. This approach led to different knowledge representations for the same concepts when the associated words changed meanings. To quantify the degree of sentiment consistency, various models for describing emotions were employed. The attention weight after considering knowledge based on sentiment consistency was represented by $\varepsilon$ and the sentiment analysis techniques for multi-word and single-word concepts were borrowed from CoreNLP \cite{pennington-etal-2014-glove} and SenticNet \cite{Cambria2020SenticNet6E}, respectively. 
Additional functions to obtain the word and position embeddings were introduced. The calculation of emotional intensity was done using pre-built methods provided by emotional models like VAD \cite{Mehrabian1996PleasurearousaldominanceAG} and Hourglass \cite{Susanto2020TheHM}, which assessed various aspects of concepts in multiple dimensions. A parameterised combination of the context and sentiment weights thus resulted in a comprehensive and context-enriched representation of the utterance.

Tu et. al. evaluated their \textit{Sentic GAT} model on three datasets -- DailyDialog \cite{Li2017DailyDialogAM}, EmoryNLP \cite{Zahiri2017EmotionDO} and MELD \cite{Poria2018MELDAM}. For the metrics, they chose the micro F1 score for the DialyDialog dataset owing to the reasons seen previously in Hazarika et. al. \cite{Hazarika2019ConversationalTL} and \cite{Chatterjee2019UnderstandingEI}. For the other two datasets, they made use of the weighted average F1 score \cite{Majumder2018DialogueRNNAA}. They used ConceptNet -- a semantic graph in which words and sentences are linked by weighted edges (with weights representing confidence scores) and labels that indicate the edge-type \cite{Speer2016ConceptNet5A}, SenticNet --  a collection of 100,000 natural language ideas for semantics, emotion, and polarity. In particular, emotion alludes to the affective value of the four dimensions - \textit{introspection, temper, attitude}, and \textit{sensitivity} of the \cite{Susanto2020TheHM}. Semantics are the ideas that are most thematically connected to input concepts. Emotional polarity values lie between -1 and +1, where -1 is very negative and +1 is very positive \cite{Cambria2020SenticNet6E}, and NRC\_VAD -- an emotional lexicon of a set of words along with their associated VAD ratings for \textit{valence, arousal}, and \textit{dominance} which $\in$ [0,1] \cite{Mohammad2018ObtainingRH}, for commonsense knowledge integration.

They compared their results with several other baseline models such as c-LSTM \cite{poria-etal-2017-context}, DialogueRNN \cite{Majumder2018DialogueRNNAA}, DialogueGCN \cite{Ghosal2019DialogueGCNAG}, KET \cite{Zhong2019KnowledgeEnrichedTF}, RGAT \cite{Ishiwatari2020RelationawareGA}, SINN \cite{Feng2021SINNAS}, MVN \cite{Ma2021AMN} and PRE-CODE \cite{Jiao2020ExploitingUD}, as well as variants of their own \textit{Sentic GAT} model, such as \textit{CANet}, \textit{Sentic GAT\_VAD}, \textit{Sentic GAT\_Hourglass} and \textit{Sentic GAT\_Intensity}. When comparing the effectiveness of the aforementioned models on the three datasets, Tu et. al. found that KET (which employed a context-aware graph attention mechanism with common-sense knowledge) performed best among the baselines. However, they argued that KET only took emotional intensity into account, not sentiment consistency. On datasets like MELD and EmoryNLP that featured lengthy discussions, the \textit{CANet} model (the author's adaptation of \textit{Sentic GAT} without the sentiment-awareness component in CSAGAT), performed well; demonstrating the usefulness of their DT and context-aware graph attention method. Overall, across various test datasets, their proposed \textit{Sentic GAT} and its variations outperformed other models (with the exception of RGAT on the MELD dataset). They hypothesised that this was due to the fact that RGAT utilised BERT \cite{Devlin2019BERTPO} for utterance encoding, which according to them was more efficient than the GloVe embeddings that they used. To evaluate the emotional intensity of concepts, they included several emotion models and sentiment polarity to their \textit{Sentic GAT\_att2\_Intensity} variant, which then did better than RGAT on DailyDialog and EmoryNLP.

The authors also presented some additional analysis and shortcomings on various parts of their framework. They first discussed the context-awareness element which was meant to establish the relationship between external concepts and the current utterance and its surrounding statements. They found that by changing the context window size, the model's accuracy and F1 score showed an increase with the addition of more context information. However, after reaching a certain threshold, the model's performance started to deteriorate. They concluded that including more context enhanced the emotion recognition but too much context bore a negative impact. They also noted that the computational cost proportionally increased with larger context window sizes. They next examined the sentiment-awareness component to rate the impact of mood congruence on emotional recognition. Its purpose was to simulate the way humans select concepts to preserve the connection between sentiments and what they say. Results were analysed by changing a preset threshold value and observing how the accuracy and F1 scores changed. The graphs revealed that the peak F1 score varied between datasets, with a threshold of 0.3 in MELD, and 0.6 and 0.7 in DailyDialog and EmoryNLP, respectively. These numbers highlighted the importance of considering the degree of sentiment consistency with related knowledge, which is often complicated by the presence of sarcasm that in turn make deducing the attention weights and emotional intensity tough. The authors described this scenario by using the example of the happy word \textit{laugh} which in the MELD dataset, was used to represent a negative sentiment. They used their tabulated results on the F1 scores to substantiate that the HMAT did a fantastic job of collecting context information, far better than RNNs due to their shorter data flow circuits. However, they noticed that despite both EmoryNLP and MELD being long conversation datasets, HMAT fared poorly on EmoryNLP while doing well on MELD. As was previously seen, the authors implied that this would happen because some utterances in the dataset that properly predict the speakers' emotions without the need for, or with just a minimal usage of, context information. Therefore, providing too much background information for these sentences would be counterproductive or even detrimental. Based on the findings, it was also clear that contrastive loss had the greatest influence on the MELD dataset's outcomes and was essential for discriminating between context-free and context-sensitive utterances. The authors conducted a series of experiments to analyse the effect of different parts of \textit{Sentic GAT} on its performance. They found that the size of context utterances in CAGAT and the alignment of sentiments between concepts and related knowledge in SAGAT significantly impacted the network's ability to capture knowledge representation. Moreover, the relationship between and within historical utterances, which was measured by HMAT, influenced the precision of emotion detection. Results indicated that in both short and long conversations, CAGAT had the largest influence on the model's outcome, providing semantically enhanced features through concept embeddings and elevating the representation of knowledge with contextual information.

Tu et. al. pointed out that the F1 scores for the various emotions in their dataset varied based on the number of samples and the unbalanced distribution of emotions. The dataset, MELD, in particular, showed a low F1 score for the emotion of \textit{disgust}. The model was effective at identifying \textit{fear}, but struggled to distinguish between emotions. It frequently misclassified \textit{fear, sadness}, and \textit{anger} as \textit{disgust}. They reasoned that this may be due to the similarities between these emotions, as they are all fell under to the umbrella of negative emotions in SenticNet, with comparable degrees of emotional intensities in the Hourglass model, and low valence and high arousal in the VAD model. Hence emotion recognition becomes challenging while dealing with analogous emotions in class-imbalanced datasets.


