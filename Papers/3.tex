\\
Li et. al. investigated the concept of emotion inference, which sought to indicate how the contents of one subject's speech affected those of the other participants involved in a discussion. This work built on the prior theory of speaker interdependence and entailed comprehending emotions from the viewpoint of the addressee or listener. To accomplish this, they relied on identifying the addressee's function and simulating their emotional relationships with other speakers. This demanded the model to have causal reasoning abilities to meet this goal \cite{Li2021EnhancingEI}.  The authors contended that while deep learning had been explored heavily for emotion recognition, emotion inference and shift-detection in natural conversations remained understudied. The study by Hasewaga et. al. \cite{hasegawa-etal-2013-predicting}, which evoked listeners' feelings in  dialogues in uncomplicated online dialogue environments with no more than two conversation turns, was the only non-neural network method that most closely resembled their interests. The originality of Li et. al.'s approach was to build on this setting, scale it to multi-turn, multi-party conversations and give it resilience.

The stem for their study was rooted in achieving conversational intellect to solve real-world challenges like dialogue planning, interpretability and empathetic agents that are context-aware and can mimic human interactions. Li et. al. used a small yet fitting example to describe this phenomenon in practice. For instance, if Person Y's car broke down and Person X asked \textit{``Would you like a lift home?''}, a chat-bot would need to formulate its response using the emotion as a trigger to replicate human-like behaviour. In order to choose an appropriate response that is consistent with this emotion, it would first need to recognise that Person Y was sad about their broken-down car. It would then need to use its emotion inference component to determine whether Person Y's mood might change once Person X offered to drive them home, which most probably would reflect Person Y conveying a happy sentiment. Another possible outcome in this scenario is for Person Y to continue to feel depressed over their broken-down car and not exhibit a shift in their emotional state, despite being extended aid. As outlined in the previous cases, it's clear that the chat-bot might choose an unsuitable or even harmful response, without the ability of infer emotions.

The authors identified \textit{persistence} and \textit{contagiousness} \cite{10.1016/j.knosys.2019.105084, hazarika-etal-2018-icon} as the core elements of emotion inference. Of the two situations in the toy example above, the former involved contagiousness, where Person Y's state changed from sad to happy as a result of Person X's answer, and the latter involved persistence, where Person X's response had no effect on Person Y at all. The \textit{DialogInfer} framework developed by Li et. al. aimed to understand the emotional state of the person being spoken to in a conversation. It combined both a sequence-based and graph-based approach to gather information about both recent and past interactions and also included a built-in module to determine if the addressee's emotional state was primarily influenced by themselves or by others. Additionally, they proposed a technique to incorporate a particular kind of inferential knowledge into their model. They argued that while machines have difficulty understanding the causes and effects of events, humans can do so with ease. They integrated COMET \cite{bosselut-etal-2019-comet}, a conversation-related common-sense transformer model trained on inferential knowledge graphs to produce diverse and enriched knowledge, to get around this issue. ConceptNet \cite{Speer2016ConceptNet5A}, a semantic network with concept-level relational knowledge, and ATOMIC, an encyclopedia of 877k event-level textual descriptions of inferential knowledge and routine common sense reasoning \cite{10.1609/aaai.v33i01.33013027}, are the two knowledge bases on which COMET was trained. By using this special empirical information, the authors hoped to strengthen the ability of their model to infer emotions.

Li et. al. formulated a prediction problem to determine the addressee $p^a_m$â€™s emotional reaction $E_m^a$ to the utterance $U_m$, in a multi-turn, multi-party conversation. The inference was modelled on the entire conversation history --
$E_m^a \sim P(E_m^a |(U_1, p^w_1 ), (U_2, p^w_2 ), . . . , (U_m, p^w_m),\allowbreak p^a_m)$, where $(U_1,U_2,...,U_t,...,U_m)$ represented the complete list of utterances in a dialogue and $U_t$ was the utterance at time-stamp $t$ consisting of $N$ words. The notation $p$ with superscript $w$ was used to identify the role of a writer/speaker, while that with superscript $a$ pointed to the addressee/listener. In cases where the addressee was influenced by their own emotions, they assumed the same identity as the writer. 

The first step in the construction of the \textit{DialogInfer} architecture was to create a context-independent emotional representation $u_t$ for each utterance $U_t$. To this end, the authors used a CNN encoder with a 300-dimensional embedding layer of the pre-trained 840B GloVe vectors \cite{pennington-etal-2014-glove}. They utilised a range of filters with sizes of 3, 4, 5, and 50 to extract these features and encode all of the dataset's phrases with their emotional content as a 100-dimensional vector, which formed the encoder network's output layer. Additionally, they tuned the parameters of the pre-trained large-architecture RoBERTa-encoder to produce 1024-dimensional representations resembling those of the CNN counterpart, after initialising it using the values suggested in \cite{https://doi.org/10.48550/arxiv.1907.11692}.

The second stage involved creating an addressee-aware sequence-model. The intuition underlying this action was supported by the fact that utterances are sequential entities in the dialogue universe. Given that LSTMs have been successfully used for sequence classification tasks, the authors used them to model all conversations in their dataset \cite{Hochreiter1997LongSM}. For each time-step $t$, an LSTM-unit consisted of an input $x_t$, hidden-state $h_t$, cell-state $c_t$, input-gate $i_t$, forget-gate $f_t$ and output-gate $o_t$. The function of this component was to handle the gradual transmission of emotions over time. It was mathematically expressed as $(h_t,c_t) = LSTM(x_t, (h_{t-1},c_{t-1}))$. The importance of the cell-state $c^a_t$ and hidden-state $h^a_t$ was more officially highlighted by their responsibilities in recording and monitoring the internal and expressed emotional states of an addressee $p^a_m$. While the information at the internal emotional state $c^a_t$ was regulated by the input and forget gates $i_t$ and $f_t$, the data that culminated in the expressed emotional state $h^a_t$ was obtained by passing the signal from $c^a_t$ into the output-gate $o_t$. Two distinct types of specially designed LSTM cell layouts, each with its own set of parameters -- $LSTM_{store}$ and $\mathit{LSTM_{affect}}$, were used to control the process flow, which came in two kinds. According to the formal definition provided by Li et. al., the $LSTM_{store}$ unit would open its input-gate and store information into the internal state $c^a_t$ whenever the addressee spoke a historical utterance $u_t$ at time stamp $t$. The addressee and the writer had the same role in this case, and therefore $p^a_m = p^w_m$. This record was ideally required to examine how the addressee's past emotional states related to their future emotion, which carried the concept of \textit{persistence}. If, on the other hand, the addressee was influenced by another speaker's utterance $u_t$ at some time stp $t$ when $p^a_m \neq p^w_m$, the $LSTM_{store}$ opened its forget-gate to release the addressee's own previous state $c^a_{t-1}$ and altered the present state $c^a_t$ with this updated new knowledge. If the addressee was unaffected by the other participant's utterance, the $LSTM_{store}$ unit closed its input-gate and retained the addressee's prior state $c^a_t$. This exemplified the concept of \textit{contagiousness}. The final expressed emotional state $h^a_m$ was obtained by sending the last internal emotional state $c^a_m$ to an output-gate $o_m$. The $h^a_m$ was then loaded into an affine-layer with a ReLU activation function to produce the final emotional representation $es^a_m$ of the addressee $p^a_m$ at some time stamp $m$. The output representation was a 100-dimensional vector.

The third phase was to construct the graph-based addressee aware model, which recorded long-term historical information as opposed to the sequence model, which only assisted in recording short-term memories. Each graph node $g_t$ was instantiated with a linearly transformed utterance representation $u_t$, obtained from the CNN and RoBERTa-encoders. The node $g_t$ was then linked to each of its preceding sibling nodes $(g_{t-1},g_{t-2},....,g_{1})$ to gather all of the historical knowledge of the encoded utterances into it, based on the edge weights. There were a total of 100 nodes in the graph. The edges of the graph represented the dialogue states' relationships. Because the inference task required forecasting the addressee's present emotion, the nodes were only connected to their antecedents and future-state look-ahead operations were forbidden. The edge weights represented the degree of the interactions between nodes using a generalised attention function -- $ATT(g_m,g_t)$. Similar to the topology of the LSTM cells in the sequence model, two refined attention functions of the form $ATT_{persist}$ and $\mathit{ATT_{affect}}$ were adopted to differentiate between persistence and contagiousness. In cases where the addressee was the speaker of an utterance $u_t$ i.e., $p^a_m = p^w_m$, $ATT_{persist}$ was used to compute the edge weights between any two nodes $g_m$ and $g_t$ while $\mathit{ATT_{affect}}$ was used otherwise. The attention weight between nodes was calculated as a weighted combination of the two functions, and the node update for any node $g_m$ in the graph was executed as a linear sum of all the connected nodes and their previously computed attention coefficients. Finally, the addressee $p^a_m$'s emotion representation $eg^a_m$ was obtained by applying the ReLU activation function over the linear transform of the updated node values. Despite the fact that the network effectively captured and held long-term dependencies, the authors discovered that they paid greater attention to semantically connected nodes based on the attention value. To avoid completely excluding nodes that were not semantically connected and hence presumably less significant in predicting the addressee's emotion, they incorporated an unassuming contextual unidirectional LSTM to aggregate both types of information and integrated this into the graph model's emotion representation. This was formalised as -- $eg^a_mâ€² = eg^a_m + LSTM(u\leq m)$

The fourth and penultimate stage was commonsense knowledge integration. Li et. al. employed the COMET model to derive three categories of inferential knowledge -- $k^{oReact}_m$, $k^{oWant}_m$ and $k^\mathit{oEffect}_m$, from the input utterance $U_m$. According to the ATOMIC framework, each input phrase was formulated as a triplet $\{s, r, o\}$ where simply put, $s$ represented the phrase's subject or event description, $r$ represented the response or sentiment the event sparked in the participants, and $o$ represented the outcome or result of $r$. The paper provides a thorough explanation of the various facets of the ATOMIC knowledge base and is worth reading. The output phrases $k^{oReact}_m$, $k^{oWant}_m$ and $k^\mathit{oEffect}_m$ were then encapsulated to deduce the emotional reaction $ek^a_m$ of the addressee $p^a_m$ using a ReLU activation function and a linear transform layer.

As the fifth and last step, the emotion representations from the sequence-model $es^a_m$, graph-network $eg^a_m$ and knowledge-integration model $ek^a_m$ were concatenated to obtain the final emotion representation $e^a_m$ which was standardised as -- $e^a_m = \lambda_1 \cdot es^a_m + \lambda_2 \cdot eg^a_m + \lambda_3 \cdot ek^a_m$. $\lambda_1 = 1.0, \lambda_2 = 1.0$ and $\lambda_3 = 0.3$ were chosen and established as the optimal empirical hyper-parameter configuration by the authors. The final inferred emotion $E^a_m$ was obtained by passing $e^a_m$ through a linear classifier with the softmax activation function and $C$ output emotional categories. The authors postulated that by using ReLU to map all of the output vectors to a positive value space, rather than experimenting with various activation functions, they were able to prevent canceling positive and negative values in the parameterised summed representation for $e^a_m$.

Li et. al. evaluated their model on the IEMOCAP \cite{Busso2008IEMOCAPIE}, MELD \cite{Poria2018MELDAM} and EmoryNLP \cite{Zahiri2017EmotionDO} datasets and compared their results with several other state-of-the-art baseline neural-network based models such as the CNN \cite{Kim2014ConvolutionalNN} with GloVe embeddings, sc-LSTM \cite{poria-etal-2017-context}, DialogueRNN \cite{Majumder2018DialogueRNNAA}, DialogueGCN \cite{Ghosal2019DialogueGCNAG}, RoBERTa \cite{https://doi.org/10.48550/arxiv.1907.11692}, RoBERTa sc-LSTM, RoBERTa DialogueRNN, COMET \cite{bosselut-etal-2019-comet} and COSMIC \cite{Ghosal2020COSMICCK}. The also tried out different versions of their model including DialogInfer-S, DialogInfer-G, DialogInfer-G + sc-LSTM, DialogInfer-(S+G) and DialogInfer-(S+G)+K. The letters S, G, and K represented sequence-based awareness of the person being addressed, graph-based awareness of the person being addressed, and external knowledge of common sense, respectively. They conducted these tests both on the GloVe and RoBERTa versions. They utilised the weighted macro F1 score as their evaluation metric and the median scores from five iterations of each model variant was considered.

The authors followed Hasewaga et. al. \cite{hasegawa-etal-2013-predicting} to create training samples for the emotion inference task. They took the first $m$ exchanges as input and the label of the $(m+1)$th one as the prediction target. Only considering the participant information of the $(m+1)$th turn, they excluded the actual text of that turn from the input as the aim was to predict the emotional state of the addressee based on the first $m$ utterances without peeking into their impending reaction. For this reason, they adapted DialogueRNN, DialogueGCN, and COSMIC to suit the emotion inference endeavor by limiting their access to information about future utterances.

The \textit{DialogInfer-(S+G)+K} variant was found to be more effective than the baseline methods based on the weighted F1 scores of 61.03 and 65.20 for the GloVe and RoBERTa versions in the IEMOCAP dataset and 39.2 and 40.96 in the MELD dataset. The results of \textit{DialogInfer-S, DialogInfer-G}, and \textit{DialogInfer-(S+G)} also showed improved performance, demonstrating the advantage of taking into account both short and long term contexts. The higher scores for RoBERTa were likely due to its pre-training on vast amounts of unstructured text, making the features it extracted more valuable. The results for the EmoryNLP dataset were different, with the \textit{DialogInfer-(S+G)} and \textit{DialogInfer-S} models performing the best with F1 scores of 21.69 and 23.09 respectively, on both the GloVe and RoBERTa versions. The reason being that the EmoryNLP dataset has a fewer utterances from multiple speakers, and in such cases, incorporating longer historical contexts or external commonsense knowledge acted as a hindrance and corrupted the efficacy of original \textit{DialogInfer} model that included all three components. They also conducted experiments individually on the \textit{DialogInfer-S, DialogInfer-G, DialogInfer-G + sc-LSTM, DialogInfer-(S+G)} variants, without including the addressee-aware component, for all three datasets. The results, for both GloVe and RoBERTa, showed a decrease in F1 scores after excluding the addressee-aware module. This indicated that the segment effectively modelled the emotional impact and emotional contagion in conversations and could learn about the  emotion shifts experienced by the participants. To determine the usefulness of commonsense knowledge for emotion inference, the authors evaluated the performance of the COMET model that relied solely on commonsense information without the need for conversation utterances. The results of such knowledge bases (COMET and COSMIC) were compared to the baseline sentential-ERC models and it was observed that the models incorporating commonsense knowledge achieved equivalent results. This showed that the utilisation of external knowledge resources added significant value to the models and verified the proficiency of the integrated knowledge constituents. Li et. al. explored the impact of their addressee-aware module in the graph-based \textit{DialogInfer-G} model by illustrating the attention weights on the IEMOCAP dataset. The depictions showed that the module gave greater consideration to instances where the speaker and addressee were the same person. This was due to emotions being a persistent state, indicating a common human predisposition to be influenced by our own recent emotional experiences. The model was able to differentiate between different inputs and assign weights appropriately. Additionally, the graph-based model was intelligent enough to surpass time limitations when required and maintain them when the utterances were semantically linked, regardless of how far in the past they were. The authors also showed that including knowledge phrases from the external source COMET, which pertained to emotional reactions, desires, and effects (\textit{oReact, oWant} and \textit{oEffect}) of participants, provided a more focused view from the listener's perspective and enhanced the ability to infer their emotions. For instance, in one of the examples, Person A said \textit{``I'm sorry your husband cheated on you''}, for which the model was able to draw on its commonsense knowledge and infer that the listener (Person B) would feel \textit{sadness, pain}, or \textit{distress} and would feel the need to weep or end their marriage. This theoretical basis helped determine the listener's emotion, which in this case is \textit{melancholy}. Moreover, in many examples that observed emotion transmission trends, the authors noted the phenomenon of persistence in closely situated utterances (lower shift rates) while transitions between feelings of the same kind (e.g. happy and excited) were more pronounced (higher shift rates).

Although the \textit{DialogInfer} model was able to beat the baseline models' F1 scores by $\approx 3-4\%$ the authors encountered some deficiencies, especially with the class imbalance problem, as was  the case with the studies we have seen so far. For example, in the MELD dataset, emotion classes such as \textit{fear} and \textit{disgust} showed a lower representation. The authors argued that determining emotions was a challenging feat as individuals tend to see events subjectively and process them in different ways, and sometimes external knowledge sources can be unreliable to handle such large disparities.




